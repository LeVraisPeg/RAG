{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMmTUA+fndnKVrVeTtgyB9m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeVraisPeg/RAG/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoBtqQwba7ID",
        "outputId": "e3ea92b4-d277-4588-c90b-7cb71b94a32e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exécution sur Google Colab\n",
            "fatal: destination path 'tp-rag-student-version' already exists and is not an empty directory.\n",
            "/content/tp-rag-student-version\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Vérifie si le notebook est exécuté sur Google Colab\n",
        "if \"COLAB_GPU\" in os.environ or \"COLAB_TPU_ADDR\" in os.environ:\n",
        "    print(\"Exécution sur Google Colab\")\n",
        "\n",
        "    # Clone du dépôt\n",
        "    !git clone https://github.com/vincentmartin/tp-rag-student-version.git\n",
        "\n",
        "    # Se placer dans le dossier du projet\n",
        "    %cd tp-rag-student-version\n",
        "\n",
        "    # Installation des dépendances\n",
        "    !pip install -r requirements.txt\n",
        "else:\n",
        "    print(\"Pas sur Google Colab\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "\n",
        "for p in DATA_DIR.rglob(\"*\"):\n",
        "    print(p)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34zJRFjTbu8T",
        "outputId": "16a7c0bb-1fe8-4964-da9c-199ed688361d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data/patents\n",
            "data/latex\n",
            "data/autres_articles\n",
            "data/arxiv\n",
            "data/patents/BANDAGE.txt\n",
            "data/patents/METHOD AND DEVICE FOR MANUFACTURING CONSTRUCTION BLOCKS FROM A HYDRAULIC BINDER SUCH AS PLASTER, AN INERT FILLER SUCH AS SAND, AND WATER.txt\n",
            "data/patents/Electromagnetic radiation monitor.txt\n",
            "data/patents/Method for deriving character features in a character recognition system.txt\n",
            "data/patents/APPARATUS FOR THE MEASUREMENT OF ATRIAL PRESSURE.txt\n",
            "data/patents/ATMSPOS BASED ELECTRONIC MAIL SYSTEM.txt\n",
            "data/patents/MICROWAVE TURNTABLE CONVECTION HEATER.txt\n",
            "data/patents/SPECIFIC DNA SEQUENCES OF A NEMATODE WHICH CAN BE USED FOR THE DIAGNOSIS OF INFECTION WITH THE NEMATODE.txt\n",
            "data/patents/CONDOM FOR ORAL-GENITAL USE.txt\n",
            "data/patents/Wear component and method of making same.txt\n",
            "data/patents/PHARMACEUTICAL COMPOSITIONS OF GALLIUM COMPLEXES OF 3-HYDROXY-4-PYRONES.txt\n",
            "data/patents/SELECTION OF RIBOZYMES THAT EFFICIENTLY CLEAVE TARGET RNA.txt\n",
            "data/patents/DEVICE FOR FORMATION OF A FILM ON THE WALLS OF HOLES IN PRINTED CIRCUIT BOARDS.txt\n",
            "data/patents/DUAL PISTON STRUT.txt\n",
            "data/patents/A PROCESS FOR REGENERATING SPENT FLUIDIZED CATALYTIC CRACKING CATALYST.txt\n",
            "data/patents/Device for the examination of flow motion inside cylindrical components.txt\n",
            "data/patents/CONDENSED BENZOXA RING COMPOUND, PRODUCTION THEREOF, AND PHARMACEUTICAL COMPOSITION CONTAINING THE SAME.txt\n",
            "data/patents/USE OF A GENE ENCODING OXALATE OXIDASE FOR THE TRANSFORMATION OF PLANTS.txt\n",
            "data/patents/SAFETY SYRINGE ASSEMBLY WITH RADIALLY DEFORMABLE BODY.txt\n",
            "data/patents/COMPOSITE BLOCK AND PROCESS FOR MANUFACTURING.txt\n",
            "data/patents/GENE EXPRESSION IN BACILLI.txt\n",
            "data/patents/DISPENSING APPARATUS UTILIZING A PRESSURE GENERATOR.txt\n",
            "data/patents/XYLANASE PRODUCTION.txt\n",
            "data/patents/DETERMINATION OF TRICYCLIC ANTIDEPRESSANT DRUGS IN THE PRESENCE OF INTERFERING SUBSTANCES.txt\n",
            "data/patents/SYSTEM FOR FIXING JUXTAPOSED AND PARALLEL SLATS.txt\n",
            "data/latex/Complex QA & language models hybrid architectures, Survey.bib\n",
            "data/latex/Multi-Agent Reinforcement Learning, Methods, Applications, Visionary Prospects, and Challenges.tex\n",
            "data/latex/Reconfigurable Intelligent Surface Assisted Railway Communications A survey.bib\n",
            "data/latex/Macroeconomic_Effects_of_Inflation_Targeting_A_Survey_of_the_Empirical__Literature.tex\n",
            "data/latex/Complex QA & language models hybrid architectures, Survey.tex\n",
            "data/latex/A_survey_on_algebraic_dilatations.bib\n",
            "data/latex/A_survey_on_the_complexity_of_learning_quantum_states.bib\n",
            "data/latex/A_survey_on_algebraic_dilatations.tex\n",
            "data/latex/A Survey of Software-Defined Smart Grid Networks, Security Threats and Defense Techniques.bib\n",
            "data/latex/Literature_Survey_on_the_Container_Stowage_Planning_Problem.bib\n",
            "data/latex/Multi-Agent Reinforcement Learning, Methods, Applications, Visionary Prospects, and Challenges.bib\n",
            "data/latex/A_survey_on_the_complexity_of_learning_quantum_states.tex\n",
            "data/latex/A_Survey_on_Blood_Pressure_Measurement_Technologies_Addressing__Potential_Sources_of_Bias.tex\n",
            "data/latex/Literature_Survey_on_the_Container_Stowage_Planning_Problem.tex\n",
            "data/latex/Reconfigurable Intelligent Surface Assisted Railway Communications A survey.tex\n",
            "data/latex/A_Survey_on_Blood_Pressure_Measurement_Technologies_Addressing__Potential_Sources_of_Bias.bib\n",
            "data/latex/A Survey of Software-Defined Smart Grid Networks, Security Threats and Defense Techniques.tex\n",
            "data/latex/Macroeconomic_Effects_of_Inflation_Targeting_A_Survey_of_the_Empirical__Literature.bib\n",
            "data/latex/A_survey_on_the_complexity_of_learning_quantum_states.txt\n",
            "data/latex/A_survey_on_algebraic_dilatations.txt\n",
            "data/autres_articles/2412.18601v1.pdf\n",
            "data/autres_articles/2412.18605v1.pdf\n",
            "data/autres_articles/2412.18608v1.pdf\n",
            "data/autres_articles/2412.18604v1.pdf\n",
            "data/autres_articles/2412.18603v1.pdf\n",
            "data/autres_articles/2412.18596v1.pdf\n",
            "data/autres_articles/2412.18600v1.pdf\n",
            "data/autres_articles/2412.18609v1.pdf\n",
            "data/autres_articles/2412.18607v1.pdf\n",
            "data/autres_articles/2412.18597v1.pdf\n",
            "data/arxiv/Multi-Agent_Reinforcement_Learning_Methods_Applications_Visionary__Prospects_and_Challenges.pdf\n",
            "data/arxiv/Reconfigurable_Intelligent_Surface_Assisted_Railway_Communications_A__survey.pdf\n",
            "data/arxiv/Literature_Survey_on_the_Container_Stowage_Planning_Problem.pdf\n",
            "data/arxiv/A_survey_on_the_complexity_of_learning_quantum_states.pdf\n",
            "data/arxiv/A_Survey_of_Software-Defined_Smart_Grid_Networks_Security_Threats_and__Defense_Techniques.pdf\n",
            "data/arxiv/A_Survey_on_Blood_Pressure_Measurement_Technologies_Addressing__Potential_Sources_of_Bias.pdf\n",
            "data/arxiv/Sketching_a_Model_on_Fisheries_Enforcement_and_Compliance_--_A_Survey.pdf\n",
            "data/arxiv/Complex_QA_and_language_models_hybrid_architectures_Survey.pdf\n",
            "data/arxiv/Macroeconomic_Effects_of_Inflation_Targeting_A_Survey_of_the_Empirical__Literature.pdf\n",
            "data/arxiv/A_survey_on_algebraic_dilatations.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-community chromadb sentence-transformers\n",
        "!pip install langchain-text-splitters\n",
        "!pip install pypdf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2jVdzPfcOhK",
        "outputId": "c15efd72-04e4-4630-926f-978ca30cd521"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.4)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.4.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.7)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.3)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.23.2)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.39.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.50.0)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (34.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.5)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.26.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: packaging>=24.0 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.43.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.13.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.39.1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (6.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-text-splitters) (1.2.7)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.3.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade \\\n",
        "  langchain langchain-core langchain-community \\\n",
        "  langchain-huggingface langchain-chroma \\\n",
        "  chromadb sentence-transformers pypdf\n"
      ],
      "metadata": {
        "id": "pumoQVTdswQS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Etape 1. - Indexation des documents"
      ],
      "metadata": {
        "id": "J0PKmFraVDYy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercice 1 : indexation"
      ],
      "metadata": {
        "id": "aAkhVBqKYu-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import TextLoader, PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "DATA_DIR = Path(\"data\")\n",
        "PERSIST_DIR = \"chroma_db\"\n",
        "COLLECTION_NAME = \"tp_rag\"\n",
        "EMBED_MODEL = \"intfloat/multilingual-e5-base\"\n"
      ],
      "metadata": {
        "id": "91DdA4Rqiis8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nettoyage des fichiers LaTeX (.tex)\n",
        "\n",
        "Le corpus contient des documents au format LaTeX. Avant l’indexation dans une base vectorielle, on réalise un **pré-traitement** pour éviter d’indexer du bruit :\n",
        "\n",
        "- suppression des lignes commentées (`% ...`)\n",
        "- suppression du préambule LaTeX (avant `\\begin{document}`)\n",
        "- suppression de tout ce qui suit `\\end{document}`\n",
        "- suppression de commandes fréquentes du \"front-matter\" (titre, auteurs, packages…)\n",
        "- normalisation des espaces et retours à la ligne\n",
        "\n",
        "Objectif : **ne conserver que le contenu textuel utile** pour améliorer la qualité des embeddings et donc du retrieval.\n"
      ],
      "metadata": {
        "id": "cwIfIpg-Vl4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "LATEX_FRONTMATTER_CMDS = [\n",
        "    \"Title\", \"TitleCitation\", \"author\", \"affil\", \"date\", \"abstract\",\n",
        "    \"keywords\", \"maketitle\", \"documentclass\", \"usepackage\"\n",
        "]\n",
        "\n",
        "def clean_latex(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Enlever lignes commentées\n",
        "    text = \"\\n\".join([ln for ln in text.splitlines() if not ln.lstrip().startswith(\"%\")])\n",
        "\n",
        "    # Si \\begin{document} existe, on garde seulement après\n",
        "    m = re.search(r\"\\\\begin\\s*\\{\\s*document\\s*\\}\", text, flags=re.IGNORECASE)\n",
        "    if m:\n",
        "        text = text[m.end():]\n",
        "\n",
        "    # Enlever tout après \\end{document} si présent\n",
        "    text = re.split(r\"\\\\end\\s*\\{\\s*document\\s*\\}\", text, flags=re.IGNORECASE)[0]\n",
        "\n",
        "\n",
        "    #clean\n",
        "    for cmd in LATEX_FRONTMATTER_CMDS:\n",
        "        text = re.sub(rf\"\\\\{cmd}\\s*\\{{.*?\\}}\", \" \", text, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    text = re.sub(r\"[ \\t]{2,}\", \" \", text)\n",
        "\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "gAoNegZsp4KZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement du corpus (TXT/MD, LaTeX, PDF)\n",
        "\n",
        "Le corpus documentaire est stocké dans le dossier `data/` et contient plusieurs formats.\n",
        "Pour pouvoir indexer tous les documents de manière uniforme, on met en place une fonction de chargement qui :\n",
        "\n",
        "- parcourt récursivement le dossier (`rglob(\"*\")`)\n",
        "- détecte le type de fichier via l’extension\n",
        "- charge le contenu avec un loader LangChain adapté :\n",
        "  - `.txt` / `.md` : `TextLoader`\n",
        "  - `.tex` : `TextLoader` + nettoyage via `clean_latex()` (suppression du bruit LaTeX)\n",
        "  - `.pdf` : `PyPDFLoader`\n",
        "\n",
        "Chaque fichier chargé est converti en objet `Document` (LangChain), qui contient :\n",
        "- `page_content` : le texte\n",
        "- `metadata` : des informations utiles (notamment `source`, c.-à-d. le chemin du fichier), très utiles ensuite pour tracer les réponses du chatbot.\n"
      ],
      "metadata": {
        "id": "5WojRyq2VujU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_all_documents(root_dir: Path) -> List[Document]:\n",
        "    docs: List[Document] = []\n",
        "\n",
        "    for path in root_dir.rglob(\"*\"):\n",
        "        if not path.is_file():\n",
        "            continue\n",
        "\n",
        "        suffix = path.suffix.lower()\n",
        "\n",
        "        # TXT\n",
        "        if suffix in [\".txt\", \".md\"]:\n",
        "            loader = TextLoader(str(path), encoding=\"utf-8\")\n",
        "            docs.extend(loader.load())\n",
        "\n",
        "        # TEX\n",
        "        elif suffix == \".tex\":\n",
        "            loader = TextLoader(str(path), encoding=\"utf-8\")\n",
        "            loaded = loader.load()\n",
        "            for d in loaded:\n",
        "                d.page_content = clean_latex(d.page_content)\n",
        "            docs.extend(loaded)\n",
        "\n",
        "        # PDF\n",
        "        elif suffix == \".pdf\":\n",
        "            loader = PyPDFLoader(str(path))\n",
        "            docs.extend(loader.load())\n",
        "\n",
        "        else:\n",
        "            # formats ignorés (.bib, etc.)\n",
        "            pass\n",
        "\n",
        "    return docs\n",
        "\n",
        "\n",
        "documents = load_all_documents(DATA_DIR)\n",
        "from collections import Counter\n",
        "\n",
        "sources = [str(d.metadata.get(\"source\",\"\")) for d in documents]\n",
        "exts = [Path(s).suffix.lower() for s in sources]\n",
        "print(\"Extensions chargées:\", Counter(exts))\n",
        "\n",
        "\n",
        "print(f\"Nombre total de documents chargés : {len(documents)}\")\n",
        "print(\"Exemple source :\", documents[0].metadata.get(\"source\") if documents else \"N/A\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU3v0zY7ilrc",
        "outputId": "7073c73a-0e52-4428-aa51-e9fb70557a6b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extensions chargées: Counter({'.pdf': 555, '.txt': 27, '.tex': 9})\n",
            "Nombre total de documents chargés : 591\n",
            "Exemple source : data/patents/BANDAGE.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Découpage des documents en paragraphes (chunking)\n",
        "\n",
        "Avant de calculer les embeddings, les documents doivent être découpés en unités plus petites appelées *chunks*.\n",
        "Dans ce TP, nous avons choisi un **découpage par paragraphes**, basé sur les doubles retours à la ligne (`\\n\\n`).\n",
        "\n",
        "Pour chaque document :\n",
        "- le texte est découpé en paragraphes\n",
        "- les paragraphes trop courts (moins de 50 caractères) sont ignorés\n",
        "- chaque paragraphe valide devient un nouveau `Document` LangChain\n",
        "\n",
        "Un identifiant `chunk_id` est ajouté dans les métadonnées afin de :\n",
        "- conserver l’ordre des paragraphes dans le document d’origine\n",
        "- permettre plus tard de regrouper les chunks provenant d’un même fichier (ex. résumé de document complet)\n"
      ],
      "metadata": {
        "id": "gloV6bQEV4CC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_by_paragraphs(docs: List[Document], min_len: int = 50) -> List[Document]:\n",
        "    out: List[Document] = []\n",
        "    for d in docs:\n",
        "        text = d.page_content or \"\"\n",
        "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
        "\n",
        "        for i, p in enumerate(paragraphs):\n",
        "            if len(p) < min_len:\n",
        "                continue\n",
        "            out.append(Document(\n",
        "                page_content=p,\n",
        "                metadata={**d.metadata, \"chunk_id\": i}\n",
        "            ))\n",
        "    return out\n",
        "\n",
        "paragraph_docs = split_by_paragraphs(documents, min_len=50)\n",
        "print(f\"Nombre total de paragraphes : {len(paragraph_docs)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIifXBMxin2O",
        "outputId": "642a57b7-2350-4002-8bd3-e826bfb379a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de paragraphes : 1914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtrage des paragraphes non pertinents (bruit et bibliographie)\n",
        "\n",
        "Après le découpage en paragraphes, tous les chunks ne sont pas utiles pour la recherche sémantique.\n",
        "Certains contiennent uniquement du bruit, par exemple :\n",
        "- bibliographies et références\n",
        "- préambule ou structure LaTeX\n",
        "- paragraphes majoritairement composés de commandes ou de commentaires\n",
        "\n",
        "Nous mettons donc en place une fonction heuristique `is_bibliography_or_noise` qui permet\n",
        "d’exclure ces paragraphes avant l’indexation.\n"
      ],
      "metadata": {
        "id": "JWV_-zTtWBCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "BIB_PATTERNS = [\n",
        "    r\"\\\\bibitem\",\n",
        "    r\"\\\\newblock\",\n",
        "    r\"\\\\begin\\{thebibliography\\}\",\n",
        "    r\"\\\\end\\{thebibliography\\}\",\n",
        "]\n",
        "NOISE_PATTERNS = [\n",
        "    r\"\\\\begin\\{document\\}\",\n",
        "    r\"\\\\end\\{document\\}\",\n",
        "    r\"\\\\documentclass\",\n",
        "    r\"\\\\usepackage\",\n",
        "    r\"\\\\title\\{\",\n",
        "    r\"\\\\author\\{\",\n",
        "    r\"\\\\maketitle\",\n",
        "]\n",
        "\n",
        "def is_bibliography_or_noise(text: str) -> bool:\n",
        "    t = (text or \"\").strip()\n",
        "    if not t:\n",
        "        return True\n",
        "\n",
        "    # Bibliographie / références\n",
        "    if any(re.search(p, t) for p in BIB_PATTERNS):\n",
        "        return True\n",
        "\n",
        "    # Préambule LaTeX / structure\n",
        "    if any(re.search(p, t) for p in NOISE_PATTERNS):\n",
        "        return True\n",
        "\n",
        "\n",
        "    lines = [ln.strip() for ln in t.splitlines() if ln.strip()]\n",
        "    if lines:\n",
        "        comment_ratio = sum(1 for ln in lines if ln.startswith(\"%\")) / len(lines)\n",
        "        if comment_ratio > 0.6:\n",
        "            return True\n",
        "\n",
        "\n",
        "    if t.count(\"\\\\\") / max(len(t), 1) > 0.03:\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "before = len(paragraph_docs)\n",
        "docs_for_index = [d for d in paragraph_docs if not is_bibliography_or_noise(d.page_content)]\n",
        "after = len(docs_for_index)\n",
        "print(f\"Filtrage: {before} -> {after} chunks conservés (supprimés: {before-after})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHr-X0a6mmB2",
        "outputId": "b79b0a83-41a2-447f-c0f1-7b897177a9f3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtrage: 1914 -> 1406 chunks conservés (supprimés: 508)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialisation du modèle d’embeddings et préparation des documents\n",
        "\n",
        "Avant de créer l’index vectoriel, nous initialisons le modèle d’embeddings\n",
        "et préparons les documents à indexer.\n",
        "\n",
        "### Réinitialisation de l’index\n",
        "Pour éviter toute incohérence liée à un index existant (changement de paramètres,\n",
        "nouveaux documents, nouveau pré-traitement), le dossier de persistance est supprimé\n",
        "s’il existe déjà.\n",
        "\n",
        "### Choix du modèle d’embeddings\n",
        "Nous utilisons le modèle **`intfloat/multilingual-e5-base`**, qui présente plusieurs avantages :\n",
        "- représentation sémantique de haute qualité\n",
        "- support multilingue\n",
        "- modèle largement utilisé pour la recherche sémantique\n",
        "\n",
        "Le calcul des embeddings est effectué :\n",
        "- sur GPU si disponible (`cuda`)\n",
        "- sinon sur CPU\n"
      ],
      "metadata": {
        "id": "_Qai2K0xWTLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil, os\n",
        "\n",
        "# Reset pour éviter de réutiliser un index sale\n",
        "if os.path.exists(PERSIST_DIR):\n",
        "    shutil.rmtree(PERSIST_DIR)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device embeddings :\", device)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=EMBED_MODEL,\n",
        "    model_kwargs={\"device\": device},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "docs_prefixed = [\n",
        "    Document(page_content=\"passage: \" + d.page_content, metadata=d.metadata)\n",
        "    for d in docs_for_index\n",
        "]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua3b61xtiuUu",
        "outputId": "18b68951-3210-4734-e15f-71c064c8b88e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device embeddings : cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Création et remplissage de la base vectorielle (ChromaDB)\n",
        "\n",
        "Une fois les documents nettoyés, découpés et transformés en embeddings,\n",
        "nous les indexons dans une base vectorielle afin de permettre la recherche sémantique.\n",
        "\n",
        "Nous utilisons **ChromaDB** comme vector store pour les raisons suivantes :\n",
        "- simplicité d’utilisation\n",
        "- persistance sur disque\n",
        "- intégration native avec LangChain\n"
      ],
      "metadata": {
        "id": "XYKmJmWuWb1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLEAN_COLLECTION_NAME = \"tp_rag_clean_v2\"\n",
        "\n",
        "\n",
        "\n",
        "vectorstore_clean = Chroma(\n",
        "    collection_name=CLEAN_COLLECTION_NAME,\n",
        "    persist_directory=PERSIST_DIR,\n",
        "    embedding_function=embeddings,\n",
        ")\n",
        "\n",
        "existing = vectorstore_clean._collection.count()\n",
        "print(\"Déjà dans l'index clean :\", existing)\n",
        "\n",
        "if existing == 0:\n",
        "    vectorstore_clean.add_documents(docs_prefixed)\n",
        "    print(\"Indexation clean terminée. Total :\", vectorstore_clean._collection.count())\n",
        "else:\n",
        "    print(\"Index clean déjà présent.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7t6sEDtiwy1",
        "outputId": "fa7cba45-93e3-483d-e64c-4890fa8b6d4e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Déjà dans l'index clean : 0\n",
            "Indexation clean terminée. Total : 1406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  query = \"query: de quoi parlent les documents ?\"\n",
        "  results = vectorstore_clean.similarity_search(query, k=3)\n",
        "\n",
        "\n",
        "  for i, d in enumerate(results, 1):\n",
        "      print(f\"\\n--- Résultat {i} ---\")\n",
        "      print(\"Source:\", d.metadata.get(\"source\"))\n",
        "      print(d.page_content[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeL9S53Miyhl",
        "outputId": "da573c94-ec43-494b-a6bb-fbbc0e45efff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Résultat 1 ---\n",
            "Source: data/latex/Complex QA & language models hybrid architectures, Survey.tex\n",
            "passage: \\citet{rogersQADatasetExplosion2022} proposes an \"evidence format\" for the explainable part of a dataset composed of Modality (Unstructured text, Semi-structured text, Structured knowledge, Images, Audio, Video, Other combinations) and Amount of evidence (Single source, Multiple sources, Pa\n",
            "\n",
            "--- Résultat 2 ---\n",
            "Source: data/latex/A Survey of Software-Defined Smart Grid Networks, Security Threats and Defense Techniques.tex\n",
            "passage: \\end{abstract}\n",
            "\\begin{IEEEkeywords}\n",
            "smart grid, software-defined networking, network security, cybersecurity\n",
            "\\end{IEEEkeywords}\n",
            "\n",
            "--- Résultat 3 ---\n",
            "Source: data/latex/Reconfigurable Intelligent Surface Assisted Railway Communications A survey.tex\n",
            "passage: \\subsection{Railway environments characteristics} %\\textcolor{red}{je trouve qu'il faudrait aller à la ligne mais je ne sais pas comment -> pas besoin, il faut laisser faire latex}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercice 2 : interrogation"
      ],
      "metadata": {
        "id": "qchiqbCsY2p0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recherche sémantique dans la base vectorielle\n",
        "\n",
        "Une fois les documents indexés dans ChromaDB, nous mettons en place une fonction\n",
        "permettant d’interroger la base à partir d’une requête utilisateur.\n",
        "\n",
        "Contrairement à une recherche par mots-clés, cette recherche est **sémantique** :\n",
        "elle repose sur la similarité entre les embeddings de la requête et ceux des documents.\n"
      ],
      "metadata": {
        "id": "kfbDvbULWlhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple\n",
        "from langchain_core.documents import Document\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "def retrieve_documents(\n",
        "    vectorstore: Chroma,\n",
        "    query: str,\n",
        "    k: int = 5\n",
        ") -> List[Tuple[Document, float]]:\n",
        "    \"\"\"\n",
        "    Interroge la base vectorielle et retourne les documents\n",
        "    les plus proches de la requête avec leur score associé.\n",
        "    \"\"\"\n",
        "\n",
        "    formatted_query = query if query.startswith(\"query:\") else f\"query: {query}\"\n",
        "\n",
        "    results = vectorstore.similarity_search_with_score(\n",
        "        formatted_query,\n",
        "        k=k\n",
        "    )\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "iO-wZ166jl_t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"railway communications\"\n",
        "results = retrieve_documents(vectorstore_clean, query, k=3)\n",
        "\n",
        "for i, (doc, score) in enumerate(results, 1):\n",
        "    print(f\"\\n--- Résultat {i} ---\")\n",
        "    print(f\"Score: {score:.4f}\")\n",
        "    print(\"Source:\", doc.metadata.get(\"source\"))\n",
        "    print(doc.page_content[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeF5DGfuqg1W",
        "outputId": "673cab03-2e48-40eb-fa35-71f8d9ab153c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Résultat 1 ---\n",
            "Score: 0.2805\n",
            "Source: data/latex/Reconfigurable Intelligent Surface Assisted Railway Communications A survey.tex\n",
            "passage: \\begin{IEEEkeywords} RIS, Railway communications, mmWave.\n",
            "\n",
            "--- Résultat 2 ---\n",
            "Score: 0.2939\n",
            "Source: data/latex/Reconfigurable Intelligent Surface Assisted Railway Communications A survey.tex\n",
            "passage: Considering the capability of RIS to solve the blockage problems in mmWave wireless communications, the use of RIS for railway communications has recently been considered as a promising candidate.\n",
            "\n",
            "--- Résultat 3 ---\n",
            "Score: 0.3151\n",
            "Source: data/latex/Reconfigurable Intelligent Surface Assisted Railway Communications A survey.tex\n",
            "passage: \\section{Conclusion}\n",
            "\\label{sec:conclusion}\n",
            "This paper presents a survey on RIS-assisted communications for railway applications, particularly in the mmWave band. First, we have defined the RIS concept, explaining its structure, and different types of RISs. A review of the various optimizat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Etape 2. - RAG"
      ],
      "metadata": {
        "id": "zD7VGI11WyBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercice 3. : prompt template"
      ],
      "metadata": {
        "id": "IDcJB3h-W1EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "RAG_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=(\n",
        "        \"Tu es un assistant de Q/R. Tu dois répondre uniquement à partir du CONTEXTE.\\n\"\n",
        "        \"Si l'information n'est pas présente dans le contexte, dis clairement: \"\n",
        "        \"\\\"Je ne sais pas d'après les documents fournis.\\\".\\n\\n\"\n",
        "        \"=== CONTEXTE ===\\n\"\n",
        "        \"{context}\\n\"\n",
        "        \"=== FIN CONTEXTE ===\\n\\n\"\n",
        "        \"Question: {question}\\n\"\n",
        "        \"Réponse (en français, claire et synthétique):\"\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "OvJpFyecwg4g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercice 4. : chaîne RAG"
      ],
      "metadata": {
        "id": "Dc2VO0jAW48C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y zstd\n",
        "\n",
        "!curl -fsSL https://ollama.ai/install.sh | sh\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH5OEv69HgBG",
        "outputId": "159871d6-cb0c-4621-d132-1a0af7120f6a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 90 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading ollama-linux-amd64.tar.zst\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve > ollama.log 2>&1 &\n"
      ],
      "metadata": {
        "id": "9SyZxLKSHw1g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://localhost:11434/api/version || echo \"Ollama DOWN\"\n",
        "!ss -ltnp | grep 11434 || echo \"port 11434 fermé\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnqZZN96MfSF",
        "outputId": "e28090fd-caf1-4fa8-d7f9-87851dba459f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ollama DOWN\n",
            "port 11434 fermé\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull qwen3:8b\n",
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZQd1Q2HICOO",
        "outputId": "9fb00a84-75ac-4461-9595-0775926f0ae4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "NAME        ID              SIZE      MODIFIED               \n",
            "qwen3:8b    500a1f067a9f    5.2 GB    Less than a second ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chaîne RAG : génération de réponse augmentée par la recherche\n",
        "\n",
        "Dans cette étape, nous mettons en œuvre une chaîne complète de **Retrieval Augmented Generation (RAG)**.\n",
        "Le principe consiste à :\n",
        "1. rechercher les passages pertinents dans la base vectorielle,\n",
        "2. construire un contexte textuel à partir de ces passages,\n",
        "3. fournir ce contexte à un modèle de langage afin de générer une réponse fidèle aux documents.\n",
        "\n",
        "Le modèle de langage utilisé est **Qwen3**, servi localement via **Ollama**.\n"
      ],
      "metadata": {
        "id": "uyxKR4q8XElX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.documents import Document\n",
        "from typing import List, Tuple\n",
        "\n",
        "def format_context(docs_with_scores: List[Tuple[Document, float]], max_chars: int = 5000) -> str:\n",
        "    parts = []\n",
        "    total = 0\n",
        "    for doc, score in docs_with_scores:\n",
        "        src = doc.metadata.get(\"source\", \"unknown_source\")\n",
        "        chunk_id = doc.metadata.get(\"chunk_id\", None)\n",
        "        header = f\"[source={src} chunk={chunk_id} score={score:.4f}]\"\n",
        "        text = doc.page_content.replace(\"passage: \", \"\").strip()\n",
        "        block = f\"{header}\\n{text}\\n\"\n",
        "        if total + len(block) > max_chars:\n",
        "            break\n",
        "        parts.append(block)\n",
        "        total += len(block)\n",
        "    return \"\\n\".join(parts)\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=\"qwen3:8b\",\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "def rag_answer(vectorstore, prompt_template, question: str, k: int = 5) -> str:\n",
        "    # 1) retrieval\n",
        "    docs_with_scores = retrieve_documents(vectorstore, question, k=k)\n",
        "\n",
        "    # 2) build context\n",
        "    context = format_context(docs_with_scores)\n",
        "\n",
        "    # 3) prompt\n",
        "    prompt = prompt_template.format(context=context, question=question)\n",
        "\n",
        "    # 4) LLM\n",
        "    resp = llm.invoke(prompt)\n",
        "    return resp.content\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlLoq0T5Gzyo",
        "outputId": "7bf8832e-d6e3-421a-b45e-02700d1e9367"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4133255840.py:20: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import ChatOllama``.\n",
            "  llm = ChatOllama(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_answer(vectorstore_clean, RAG_PROMPT, \"De quoi parle le document sur les railway communications ?\", k=5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp1RSDTJG9fH",
        "outputId": "fb07bcd7-53f7-426b-ab92-71832921b866"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le document présente une revue des communications ferroviaires assistées par les surfaces intelligentes réconfigurables (RIS), notamment dans la bande millimétrique (mmWave). Il explore les concepts de RIS, les algorithmes d'optimisation, la résolution du problème de blocage des ondes mmWave, et les applications dans les trains à grande vitesse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercice 5. : mémoire"
      ],
      "metadata": {
        "id": "whd7aOTUXN8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reformulation des questions et gestion de la mémoire\n",
        "\n",
        "Ce code met en place un mécanisme de mémoire conversationnelle pour un chatbot RAG.\n",
        "\n",
        "Il conserve l’historique des échanges entre l’utilisateur et l’assistant et l’utilise pour reformuler\n",
        "les questions de suivi en questions complètes et autonomes.\n",
        "\n",
        "Concrètement :\n",
        "- l’historique de la conversation est converti en texte exploitable par le modèle,\n",
        "- une question de suivi est reformulée à l’aide de l’historique afin d’en expliciter le sujet,\n",
        "- la recherche vectorielle est effectuée à partir de cette question reformulée,\n",
        "- le contexte récupéré et l’historique sont injectés dans le prompt de génération,\n",
        "- la réponse produite est ajoutée à l’historique pour les tours suivants.\n",
        "\n",
        "Ce mécanisme permet au chatbot de gérer des échanges multi-tours tout en conservant\n",
        "une recherche documentaire cohérente à chaque interaction.\n"
      ],
      "metadata": {
        "id": "JC-4anU4X_I8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "CONDENSE_QUESTION_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"question\"],\n",
        "    template=(\n",
        "        \"Tu reçois un HISTORIQUE de conversation et une question de suivi.\\n\"\n",
        "        \"Réécris la question de suivi en une question autonome, claire et complète, \"\n",
        "        \"en français, en intégrant les informations nécessaires depuis l'historique.\\n\\n\"\n",
        "        \"=== HISTORIQUE ===\\n\"\n",
        "        \"{chat_history}\\n\"\n",
        "        \"=== FIN HISTORIQUE ===\\n\\n\"\n",
        "        \"Question de suivi: {question}\\n\\n\"\n",
        "        \"Question autonome:\"\n",
        "    )\n",
        ")\n",
        "\n",
        "def render_chat_history(history: list, max_turns: int = 8) -> str:\n",
        "    # history = [{\"role\":\"user\",\"content\":...}, {\"role\":\"assistant\",\"content\":...}, ...]\n",
        "    trimmed = history[-2*max_turns:]\n",
        "    lines = []\n",
        "    for m in trimmed:\n",
        "        role = \"Utilisateur\" if m[\"role\"] == \"user\" else \"Assistant\"\n",
        "        lines.append(f\"{role}: {m['content']}\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def rewrite_question(question: str, history: list) -> str:\n",
        "    chat_history_str = render_chat_history(history)\n",
        "    prompt = CONDENSE_QUESTION_PROMPT.format(\n",
        "        chat_history=chat_history_str,\n",
        "        question=question\n",
        "    )\n",
        "    standalone = llm.invoke(prompt).content.strip()\n",
        "    return standalone\n",
        "\n",
        "def rag_chat(vectorstore, question: str, history: list, k: int = 5) -> str:\n",
        "    # Rewrite question si historique non vide\n",
        "    if history:\n",
        "        standalone_question = rewrite_question(question, history)\n",
        "    else:\n",
        "        standalone_question = question\n",
        "\n",
        "    # Retrieval sur question autonome\n",
        "    docs_with_scores = retrieve_documents(vectorstore, standalone_question, k=k)\n",
        "    context = format_context(docs_with_scores)\n",
        "    chat_history_str = render_chat_history(history)\n",
        "\n",
        "    # Generation\n",
        "    prompt = RAG_PROMPT.format(\n",
        "        chat_history=chat_history_str,\n",
        "        context=context,\n",
        "        question=standalone_question\n",
        "    )\n",
        "    resp = llm.invoke(prompt).content\n",
        "\n",
        "    # Update memory (on stocke la question originale, et la réponse)\n",
        "    history.append({\"role\": \"user\", \"content\": question})\n",
        "    history.append({\"role\": \"assistant\", \"content\": resp})\n",
        "    return resp\n"
      ],
      "metadata": {
        "id": "JRH0glnsG_O1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = []\n",
        "print(rag_chat(vectorstore_clean, \"Parle-moi des railway communications.\", history, k=5))\n",
        "\n",
        "print(rag_chat(vectorstore_clean, \"Et quelles sont les limites mentionnées précédement ?\", history, k=5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HNZmsL3IxKN",
        "outputId": "87550c24-5754-4a05-82f2-8bf16b6c9582"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les communications ferroviaires utilisent des technologies comme les ondes millimétriques (mmWave) pour la transmission de données. Les surfaces intelligentes réconfigurables (RIS) sont proposées comme solution prometteuse pour surmonter les problèmes de blocage des signaux mmWave, notamment dans les environnements ferroviaires. Elles permettent d'optimiser la propagation des signaux, notamment pour les trains à grande vitesse, en modulant les conditions du canal. Des recherches récentes explorent leur application dans ce domaine, avec des perspectives pour améliorer la connectivité et la fiabilité des réseaux ferroviaires.\n",
            "Les communications ferroviaires mentionnées font face à des limites liées à la sensibilité des signaux mmWave au blocage, en raison de leur forte perte de pénétration, ce qui réduit leur couverture. Les surfaces intelligentes réconfigurables (RIS) sont présentées comme une solution prometteuse pour pallier ces problèmes en améliorant l'efficacité et la portée des communications ferroviaires à haute vitesse.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercice 6- nouveaux outils"
      ],
      "metadata": {
        "id": "EwuhSaLAYKVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Récupération et résumé d’un document complet\n",
        "\n",
        "Ce code permet de récupérer un document complet à partir de la base vectorielle et d’en produire un résumé.\n",
        "\n",
        "Il regroupe tous les chunks appartenant à un même fichier source (`metadata.source`) afin de reconstruire\n",
        "le contenu du document d’origine.\n",
        "\n",
        "Concrètement :\n",
        "- les chunks correspondant à un même fichier sont récupérés depuis ChromaDB via un filtre sur les métadonnées,\n",
        "- les chunks sont triés selon leur `chunk_id` pour respecter l’ordre du document,\n",
        "- le préfixe `\"passage: \"` ajouté lors de l’indexation est supprimé,\n",
        "- les chunks sont concaténés pour reconstituer le texte complet du document,\n",
        "- un prompt de résumé est appliqué au texte reconstitué,\n",
        "- le modèle de langage génère un résumé structuré sous forme de puces avec des mots-clés finaux.\n",
        "\n",
        "Ce mécanisme constitue un nouvel outil permettant de produire un résumé global d’un document\n",
        "à partir des données indexées dans le système RAG.\n"
      ],
      "metadata": {
        "id": "bkdmqzy2YMRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "\n",
        "def get_full_document_by_source(vectorstore: Chroma, source_path: str, max_chunks: int = 200) -> str:\n",
        "    \"\"\"\n",
        "    Récupère (approximativement) un document complet en regroupant les chunks\n",
        "    ayant metadata.source == source_path.\n",
        "    \"\"\"\n",
        "    # Accès bas niveau à la collection Chroma\n",
        "    coll = vectorstore._collection\n",
        "\n",
        "    # where filtre sur metadata\n",
        "    res = coll.get(\n",
        "        where={\"source\": source_path},\n",
        "        include=[\"documents\", \"metadatas\"]\n",
        "    )\n",
        "\n",
        "    docs = res.get(\"documents\", [])\n",
        "    metas = res.get(\"metadatas\", [])\n",
        "    # Tri par chunk_id si présent\n",
        "    paired = list(zip(docs, metas))\n",
        "    paired.sort(key=lambda x: x[1].get(\"chunk_id\", 0) if isinstance(x[1], dict) else 0)\n",
        "\n",
        "    # dépréfixe \"passage: \"\n",
        "    joined = \"\\n\\n\".join([d.replace(\"passage: \", \"\").strip() for d, _ in paired[:max_chunks]])\n",
        "    return joined.strip()\n",
        "\n",
        "SUMMARY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"document\"],\n",
        "    template=(\n",
        "        \"Résume le document suivant en français.\\n\"\n",
        "        \"- Fais 8 à 12 puces.\\n\"\n",
        "        \"- Termine par 3 mots-clés.\\n\\n\"\n",
        "        \"DOCUMENT:\\n{document}\\n\"\n",
        "    )\n",
        ")\n",
        "\n",
        "def summarize_source(vectorstore: Chroma, source_path: str) -> str:\n",
        "    full_text = get_full_document_by_source(vectorstore, source_path)\n",
        "    if not full_text:\n",
        "        return \"Document introuvable pour cette source.\"\n",
        "    prompt = SUMMARY_PROMPT.format(document=full_text[:12000])  # garde-fou longueur\n",
        "    return llm.invoke(prompt).content\n"
      ],
      "metadata": {
        "id": "x4K4myrdI0mC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = retrieve_documents(vectorstore_clean, \"railway communications\", k=1)\n",
        "src = results[0][0].metadata.get(\"source\")\n",
        "print(\"Source choisie:\", src)\n",
        "print(summarize_source(vectorstore_clean, src))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACDUBmbbKcXy",
        "outputId": "93d7245e-248e-4d61-cbad-16b9d877fedd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source choisie: data/latex/Reconfigurable Intelligent Surface Assisted Railway Communications A survey.tex\n",
            "- **Contexte** : La demande croissante de passagers et de débits de données élevés pour les technologies comme le streaming vidéo et l’IoT pousse à l’exploration des fréquences millimétriques (mmWave) pour les communications ferroviaires.  \n",
            "- **Défis** : Les mmWave sont sensibles aux obstacles et souffrent de perte de propagation élevée, limitant leur couverture. Les surfaces intelligentes réconfigurables (RIS) offrent une solution prometteuse.  \n",
            "- **RIS : définition** : Structure électromagnétique réconfigurable qui transforme le canal de propagation en environnement radio programmable, améliorant la qualité du signal et la couverture.  \n",
            "- **Types de RIS** : Passif (réflexion sans amplification), actif (avec amplification), et hybride (combinaison des deux) pour optimiser l’énergie et les performances.  \n",
            "- **Optimisation** : Algorithmes comme la transformation de dualité lagrangienne permettent d’optimiser les déphasages pour des systèmes MIMO et MISO.  \n",
            "- **Réflexion paradigmes** : La réflexion spéculaire (proche du RIS) et la réflexion diffusée (lointaine) influencent la perte de chemin et la conception des systèmes.  \n",
            "- **Performances** : Les RIS passifs offrent un gain proportionnel à $N^2$ (nombre d’éléments), mais les RIS actifs améliorent le débit (jusqu’à 67 % selon les simulations).  \n",
            "- **Modèles et simulations** : Des modèles de canal réalistes et des outils comme NYUSIM sont essentiels pour évaluer les performances des systèmes RIS.  \n",
            "- **Défis futurs** : Gestion de la consommation énergétique, optimisation des modèles de canal, et étude des comportements en champ proche.  \n",
            "- **Applications ferroviaires** : Les RIS permettent de contourner les obstacles et d’étendre la couverture dans les environnements urbains.  \n",
            "\n",
            "**Mots-clés** : RIS, Communications ferroviaires, mmWave.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Etape 3 - IHM"
      ],
      "metadata": {
        "id": "YOrINBEnYTgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interface utilisateur Gradio\n",
        "\n",
        "Ce code met en place une interface utilisateur graphique permettant d’interagir avec le chatbot RAG.\n",
        "\n",
        "L’interface est construite avec Gradio et contient :\n",
        "- une zone de discussion affichant les échanges utilisateur / assistant,\n",
        "- un champ de saisie pour la question utilisateur,\n",
        "- un curseur permettant de choisir le nombre de documents récupérés (`top-k`),\n",
        "- un bouton pour envoyer un message,\n",
        "- un bouton pour réinitialiser la conversation.\n",
        "\n",
        "Le composant `gr.State` est utilisé pour stocker l’historique de la conversation.\n",
        "Cet historique est transmis à chaque appel afin de conserver la mémoire entre les messages.\n",
        "\n",
        "À chaque interaction :\n",
        "- la question utilisateur est envoyée à la fonction `rag_chat`,\n",
        "- la réponse générée est ajoutée à l’historique,\n",
        "- l’ensemble des échanges est affiché dans le composant `Chatbot`.\n",
        "\n",
        "Le bouton *Reset* permet de vider l’historique et de recommencer une nouvelle conversation.\n",
        "L’option `share=True` rend l’interface accessible via une URL publique.\n"
      ],
      "metadata": {
        "id": "LTuguUMEYgn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n"
      ],
      "metadata": {
        "id": "Lxho91oRKdTy"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def gradio_chat(user_message, k, state):\n",
        "    #historique\n",
        "    if state is None:\n",
        "        state = []\n",
        "\n",
        "    answer = rag_chat(vectorstore_clean, user_message, state, k=int(k))\n",
        "\n",
        "\n",
        "    pairs = []\n",
        "    for i in range(0, len(state), 2):\n",
        "        u = state[i][\"content\"]\n",
        "        a = state[i+1][\"content\"]\n",
        "        pairs.append((u, a))\n",
        "\n",
        "    return pairs, state\n",
        "\n",
        "def reset_chat():\n",
        "    return [], []\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# TP RAG — Chatbot (Chroma + multilingual-e5-base + Qwen3 via Ollama)\")\n",
        "    k = gr.Slider(1, 10, value=5, step=1, label=\"Top-K documents\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox(label=\"Votre question\")\n",
        "\n",
        "    state = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        send = gr.Button(\"Envoyer\")\n",
        "        clear = gr.Button(\"Reset\")\n",
        "\n",
        "    send.click(fn=gradio_chat, inputs=[msg, k, state], outputs=[chatbot, state])\n",
        "    clear.click(fn=reset_chat, inputs=[], outputs=[chatbot, state])\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "V5-cFhqBKtkm",
        "outputId": "17db8d2d-c5db-46b0-d937-742c6e69a010"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3218206157.py:25: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n",
            "/tmp/ipython-input-3218206157.py:25: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://30369d68c4e395f533.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://30369d68c4e395f533.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://30369d68c4e395f533.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6GnFNLu6b2-s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}